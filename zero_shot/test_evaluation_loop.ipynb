{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/miniconda3/envs/diffusers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-05 00:35:03.300186: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-05 00:35:03.340698: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-05 00:35:03.986199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from algorithms.feature_extraction_loading import FeatureDataset, extract_diffusion_features, feature_collate_fn, concatenate_video_features\n",
    "from evaluation.visualization import safe_heatmap_as_gif, place_marker_in_frames\n",
    "\n",
    "from evaluation.evaluation_datasets import compute_tapvid_metrics\n",
    "\n",
    "from algorithms.heatmap_generator import HeatmapGenerator\n",
    "from algorithms.zero_shot_tracker import ZeroShotTracker\n",
    "\n",
    "heatmap_generator = HeatmapGenerator()\n",
    "zero_shot_tracker = ZeroShotTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_diffusion_features(input_dataset_paths={'tapvid_davis': '../tapvid_davis/tapvid_davis.pkl'}, diffusion_model_path='../text-to-video-ms-1.7b/', restrict_frame_size=False, max_frame_size=2**18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'occlusion_accuracy': array([1.]), 'pts_within_1': array([0.0652232]), 'jaccard_1': array([0.03371097]), 'pts_within_2': array([0.20707476]), 'jaccard_2': array([0.11549548]), 'pts_within_4': array([0.45380023]), 'jaccard_4': array([0.29349392]), 'pts_within_8': array([0.65962755]), 'jaccard_8': array([0.49212258]), 'pts_within_16': array([0.78027265]), 'jaccard_16': array([0.63971072]), 'average_jaccard': array([0.31490673]), 'average_pts_within_thresh': array([0.43319968])}\n",
      "0.4331996792301524\n"
     ]
    }
   ],
   "source": [
    "mode = \"pca\"\n",
    "\n",
    "feature_dataset = FeatureDataset(feature_dataset_path='output/features/davis')\n",
    "feature_loader = DataLoader(feature_dataset, batch_size=1, collate_fn=feature_collate_fn)\n",
    "\n",
    "video_idx = 0\n",
    "\n",
    "tracking_accuracy = []\n",
    "\n",
    "for batch in feature_loader:\n",
    "    batch_query_points = []\n",
    "    batch_gt_occluded = []\n",
    "    batch_gt_tracks = []\n",
    "    batch_pred_tracks = []\n",
    "\n",
    "    sample = batch[0]\n",
    "        \n",
    "    if mode == \"pca\":\n",
    "        #concat_downblock = concatenate_video_features(\n",
    "        #    {\n",
    "        #        'down_block': sample['features']['down_block'][:]\n",
    "        #    },\n",
    "        #    perform_pca = True,\n",
    "        #    n_components = 10\n",
    "        #)\n",
    "        #concat_midblock = concatenate_video_features(\n",
    "        #    {\n",
    "        #        'mid_block': sample['features']['mid_block'][:]\n",
    "        #    },\n",
    "        #    perform_pca = True,\n",
    "        #    n_components = 10\n",
    "        #)\n",
    "        concat_upblock = concatenate_video_features(\n",
    "            {\n",
    "                'up_block': sample['features']['up_block'][0:3]\n",
    "            },\n",
    "            perform_pca = True,\n",
    "            n_components = 20\n",
    "        )\n",
    "        concat_decoderblock = concatenate_video_features(\n",
    "            {\n",
    "                'decoder_block': sample['features']['decoder_block'][1:2]\n",
    "            },\n",
    "            perform_pca = True,\n",
    "            n_components = 10\n",
    "        )\n",
    "    \n",
    "    elif mode == \"pooling\":\n",
    "        concat_downblock = concatenate_video_features(\n",
    "            {\n",
    "                'down_block': sample['features']['down_block'][:]\n",
    "            },\n",
    "            perform_pooling = True\n",
    "        )\n",
    "        concat_midblock = concatenate_video_features(\n",
    "            {\n",
    "                'mid_block': sample['features']['mid_block'][:]\n",
    "            },\n",
    "            perform_pooling = True\n",
    "        )\n",
    "        concat_upblock = concatenate_video_features(\n",
    "            {\n",
    "                'up_block': sample['features']['up_block'][:]\n",
    "            },\n",
    "            perform_pooling = True\n",
    "        )\n",
    "        concat_decoderblock = concatenate_video_features(\n",
    "            {\n",
    "                'decoder_block': sample['features']['decoder_block'][:]\n",
    "            },\n",
    "            perform_pooling = True\n",
    "        )\n",
    "\n",
    "    video_features = concatenate_video_features(\n",
    "        {\n",
    "            #'down_block': [concat_downblock],\n",
    "            #'mid_block': [concat_midblock],\n",
    "            'up_block': [concat_upblock],\n",
    "            'decoder_block': [concat_decoderblock],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    query_points = torch.cat([torch.from_numpy(p).unsqueeze(0) for p in sample['query_points'][0]], dim=0)\n",
    "\n",
    "    batch_query_points.append(query_points)\n",
    "\n",
    "    occluded = sample['occluded'][0]\n",
    "    batch_gt_occluded.append(occluded)\n",
    "\n",
    "    gt_track = sample['target_points'][0]\n",
    "    batch_gt_tracks.append(gt_track)\n",
    "\n",
    "    folder_path = os.path.join('output', 'video_' + str(video_idx))\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "    os.makedirs(folder_path)\n",
    "    query_point_file_name = os.path.join(folder_path, 'query_point.txt')\n",
    "    with open(query_point_file_name, 'w') as query_point_file:\n",
    "        query_point_file.write(str(query_points))\n",
    "        query_point_file.close()\n",
    "\n",
    "    heatmaps = heatmap_generator.generate(video_features, query_points, device=\"cpu\")\n",
    "    pred_track = zero_shot_tracker.track(heatmaps)\n",
    "\n",
    "    batch_pred_tracks.append(pred_track[..., [1,0]])\n",
    "    gt_track = gt_track[..., [1, 0]]\n",
    "\n",
    "    place_marker_in_frames(frames=sample['video'].squeeze(), tracks=pred_track[0].unsqueeze(0), occluded=occluded[0][None], ground_truth_tracks=gt_track[0], folder_path=folder_path)\n",
    "    safe_heatmap_as_gif(heatmaps, True, sample['video'].squeeze(), folder_path=folder_path)\n",
    "\n",
    "    video_idx += 1\n",
    "\n",
    "    metrics = compute_tapvid_metrics(query_points=np.array(batch_query_points), gt_occluded=np.array(batch_gt_occluded), gt_tracks=np.array(batch_gt_tracks), pred_occluded=np.array(batch_gt_occluded), pred_tracks=np.array(batch_pred_tracks), query_mode='strided')\n",
    "    print(metrics)\n",
    "\n",
    "    metrics_file_name = os.path.join(folder_path, 'metrics.txt')\n",
    "    with open(metrics_file_name, 'w') as metrics_file:\n",
    "        metrics_file.write(str(metrics))\n",
    "\n",
    "    tracking_accuracy.append(metrics['average_pts_within_thresh'])\n",
    "\n",
    "print(np.mean(tracking_accuracy))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['up_block', 'down_block', 'mid_block', 'decoder_block'])\n",
      "up_block\n",
      "avg:\n",
      "tensor(-0.1026, dtype=torch.float16)\n",
      "max:\n",
      "tensor(305., dtype=torch.float16)\n",
      "down_block\n",
      "avg:\n",
      "tensor(-0.2408, dtype=torch.float16)\n",
      "max:\n",
      "tensor(70.6250, dtype=torch.float16)\n",
      "mid_block\n",
      "avg:\n",
      "tensor(-0.2261, dtype=torch.float16)\n",
      "max:\n",
      "tensor(78.2500, dtype=torch.float16)\n",
      "decoder_block\n",
      "avg:\n",
      "tensor(0.4607, dtype=torch.float16)\n",
      "max:\n",
      "tensor(6612., dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Check mean and max of featuremaps before pca\n",
    "\n",
    "featuremapssss = sample[\"features\"]\n",
    "print(featuremapssss.keys())\n",
    "\n",
    "for (key, value) in sample[\"features\"].items():\n",
    "    concat_block = concatenate_video_features(\n",
    "        {\n",
    "            'x': value[:]\n",
    "        }\n",
    "    )\n",
    "    print(key)\n",
    "    print(\"avg:\")\n",
    "    print(torch.mean(concat_block))\n",
    "    print(\"max:\")\n",
    "    print(torch.max(concat_block))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (489999786.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 25\u001b[0;36m\u001b[0m\n\u001b[0;31m    HIER DECODER REINLADEN UND EIN FORWARDPASS\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Use decoder for upsampling\n",
    "\n",
    "feature_dataset = FeatureDataset(feature_dataset_path='output/features/davis')\n",
    "feature_loader = DataLoader(feature_dataset, batch_size=1, collate_fn=feature_collate_fn)\n",
    "\n",
    "video_idx = 0\n",
    "\n",
    "for batch in feature_loader:\n",
    "    query_points = []\n",
    "    gt_occluded = []\n",
    "    gt_tracks = []\n",
    "    pred_tracks = []\n",
    "\n",
    "    for sample in batch:\n",
    "          \n",
    "        concat_upblock = concatenate_video_features(\n",
    "            {\n",
    "                'up_block': sample['features']['up_block'][:]\n",
    "            },\n",
    "            perform_pca = True,\n",
    "            n_components = 4\n",
    "        )\n",
    "\n",
    "        ###\n",
    "        HIER DECODER REINLADEN UND EIN FORWARDPASS\n",
    "        ###\n",
    "\n",
    "        #idx = random.randint(0, len(sample['query_points'][0]) - 1)\n",
    "        query_points = sample['query_points'][0][idx]\n",
    "        query_points.append(query_points[None, :])\n",
    "\n",
    "        occluded = sample['occluded'][0, idx]\n",
    "        gt_occluded.append(occluded[None])\n",
    "\n",
    "        gt_track = sample['target_points'][0, idx]\n",
    "        gt_tracks.append(gt_track[None])\n",
    "\n",
    "        folder_path = os.path.join('output', 'video_' + str(video_idx))\n",
    "        if os.path.exists(folder_path):\n",
    "            shutil.rmtree(folder_path)\n",
    "        os.makedirs(folder_path)\n",
    "        query_point_file_name = os.path.join(folder_path, 'query_point.txt')\n",
    "        with open(query_point_file_name, 'w') as query_point_file:\n",
    "            query_point_file.write(str(query_points))\n",
    "\n",
    "        target = torch.tensor([[int(query_points[0]), query_points[1], query_points[2]]]) # Targets are now tensor in shape Nx3\n",
    "        heatmaps = heatmap_generator.generate(video_features, target, device=\"cpu\")\n",
    "\n",
    "        pred_track = zero_shot_tracker.track(heatmaps)\n",
    "\n",
    "        pred_tracks.append(pred_track.numpy()[None])\n",
    "\n",
    "        gt_track_switched = np.zeros_like(gt_track)\n",
    "        gt_track_switched[:, 1] = gt_track[:, 0]\n",
    "        gt_track_switched[:, 0] = gt_track[:, 1]\n",
    "\n",
    "        v.place_marker_in_frames(sample['video'].squeeze(), pred_track, ground_truth_tracks=gt_track_switched, folder_path=folder_path)\n",
    "        v.safe_heatmap_as_gif(heatmaps, True, sample['video'].squeeze(), folder_path=folder_path)\n",
    "\n",
    "        video_idx += 1\n",
    "\n",
    "    #metrics = compute_tapvid_metrics(query_points=np.array(query_points), gt_occluded=np.array(gt_occluded), gt_tracks=np.array(gt_tracks), pred_occluded=np.array(gt_occluded), pred_tracks=np.array(pred_tracks), query_mode='strided')\n",
    "\n",
    "    #print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
