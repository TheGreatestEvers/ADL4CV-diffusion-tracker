{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonevers/miniconda3/envs/adl4cv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from algorithms.feature_extraction_loading import FeatureDataset\n",
    "from algorithms.utils import feature_collate_fn\n",
    "from learning_based.weighted_features_tracker import WeightedFeaturesTracker, WeightedHeatmapsTracker\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data\n",
    "\n",
    "dataset = FeatureDataset(\"features/davis/\")\n",
    "dataloader = DataLoader(dataset, 1, shuffle=True, collate_fn=feature_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_batch(query_points, target_points, occluded, trackgroup, batch_size=8, shuffle=True, drop_last=False):\n",
    "    \"\"\"\n",
    "    Yields a tuple containing one batch of query_points, target_points, occluded, trackgroup\n",
    "    \"\"\"\n",
    "    num_points = query_points.shape[0]\n",
    "\n",
    "    if shuffle:\n",
    "        permutation = np.random.permutation(num_points)\n",
    "        query_points = query_points[permutation]\n",
    "        target_points = target_points[permutation]\n",
    "        occluded = occluded[permutation]\n",
    "        trackgroup = trackgroup[permutation]\n",
    "\n",
    "    if drop_last:\n",
    "        num_batches = num_points // batch_size\n",
    "    else:\n",
    "        num_batches = ceil(num_points / batch_size)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start = i*batch_size\n",
    "        end = min((i+1)*batch_size, num_points)\n",
    "\n",
    "        yield (\n",
    "            query_points[start:end],\n",
    "            target_points[start:end],\n",
    "            occluded[start:end],\n",
    "            trackgroup[start:end],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train input\n",
    "device = \"cpu\"\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "    data = data[0]\n",
    "\n",
    "    ### Overfitting to single data point\n",
    "    data[\"query_points\"] = data[\"query_points\"][:,:1,:]\n",
    "    data[\"target_points\"] = data[\"target_points\"][:,:1,:]\n",
    "    data[\"occluded\"] = data[\"occluded\"][:,:1]\n",
    "    data[\"trackgroup\"] = data[\"trackgroup\"][:,:1]\n",
    "\n",
    "    feature_dict = data['features']\n",
    "    for block_name, block_feat_list in feature_dict.items():\n",
    "        for i in range(len(block_feat_list)):\n",
    "            feature_dict[block_name][i] = feature_dict[block_name][i].to(dtype=torch.float64).to(device)\n",
    "\n",
    "    for query_batch in get_query_batch(data[\"query_points\"][0], data[\"target_points\"][0], data[\"occluded\"][0], data[\"trackgroup\"][0]):\n",
    "\n",
    "        query_points, target_points, occluded, trackgroup = query_batch\n",
    "\n",
    "        query_points = torch.tensor(query_points, dtype=torch.float32, device=device)\n",
    "        target_points = torch.tensor(target_points[..., [1, 0]], dtype=torch.float32, device=device)\n",
    "        occluded = torch.tensor(occluded, dtype=torch.float32, device=device)\n",
    "        trackgroup = torch.tensor(trackgroup, dtype=torch.float32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_block\n",
      "torch.Size([50, 10, 8, 8])\n",
      "torch.Size([50, 10, 16, 16])\n",
      "torch.Size([50, 10, 32, 32])\n",
      "torch.Size([50, 10, 32, 32])\n",
      "down_block\n",
      "torch.Size([50, 10, 16, 16])\n",
      "torch.Size([50, 10, 8, 8])\n",
      "torch.Size([50, 10, 4, 4])\n",
      "torch.Size([50, 10, 4, 4])\n",
      "mid_block\n",
      "torch.Size([50, 10, 4, 4])\n",
      "decoder_block\n",
      "torch.Size([50, 10, 64, 64])\n",
      "torch.Size([50, 10, 128, 128])\n",
      "torch.Size([50, 10, 256, 256])\n",
      "torch.Size([50, 10, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for name, blocklist in feature_dict.items():\n",
    "    print(name)\n",
    "    for map in blocklist:\n",
    "        print(map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to check gradients of\n",
    "from torchvision.transforms.functional import resize \n",
    "from algorithms.heatmap_generator import HeatmapGenerator\n",
    "from algorithms.zero_shot_tracker import ZeroShotTracker\n",
    "from algorithms.feature_extraction_loading import concatenate_video_features\n",
    "\n",
    "def fwd_func(w1, w2, w3, w4):\n",
    "\n",
    "    # list_up = feature_dict[\"up_block\"]\n",
    "    # list_down = feature_dict[\"down_block\"]\n",
    "    # list_mid = feature_dict[\"mid_block\"]\n",
    "    # list_decoder = feature_dict[\"decoder_block\"]\n",
    "      \n",
    "    # list_scaled_up = [weight * feature_maps for weight, feature_maps in zip(w1, list_up)]\n",
    "    # list_scaled_down = [weight * feature_maps for weight, feature_maps in zip(w2, list_down)]\n",
    "    # list_scaled_mid = [weight * feature_maps for weight, feature_maps in zip(w3, list_mid)]\n",
    "    # list_scaled_decoder = [weight * feature_maps for weight, feature_maps in zip(w4, list_decoder)]\n",
    "\n",
    "    # output = list_scaled_up[0]\n",
    "\n",
    "    # feature_dict_output = {\"up_block\": list_scaled_up}\n",
    "\n",
    "    \n",
    "    feature_dict_output = feature_dict.copy()\n",
    "\n",
    "    w = (w1, w2, w3, w4)\n",
    "    for i, (block_name, block_feature_list) in enumerate(feature_dict_output.items()):\n",
    "       feature_dict_output[block_name] = [weight * feature_maps for weight, feature_maps in zip(w[i], block_feature_list)]\n",
    "\n",
    "    concat_features = concatenate_video_features(feature_dict)\n",
    "    \n",
    "    heatmap_generator = HeatmapGenerator()\n",
    "    query_points_copy = query_points.clone()\n",
    "    hmps = heatmap_generator.generate(concat_features, query_points_copy)\n",
    "\n",
    "    tracker = ZeroShotTracker()\n",
    "    tracks = tracker.track(hmps)\n",
    "\n",
    "    target_points_copy = target_points.clone()\n",
    "\n",
    "    loss = torch.nn.MSELoss()\n",
    "    output = loss(tracks, target_points_copy)\n",
    "\n",
    "    print(output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(449.1927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check gradient\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "w1 = torch.tensor([1, 1, 1, 1], dtype=torch.float64, device=device, requires_grad=True)\n",
    "w2 = torch.tensor([1, 1, 1, 1], dtype=torch.float64, device=device, requires_grad=True)\n",
    "w3 = torch.tensor([1,], dtype=torch.float64, device=device, requires_grad=True)\n",
    "w4 = torch.tensor([1, 1, 1, 1], dtype=torch.float64, device=device, requires_grad=True)\n",
    "\n",
    "\n",
    "test = gradcheck(fwd_func, (w1, w2, w3, w4))\n",
    "test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
