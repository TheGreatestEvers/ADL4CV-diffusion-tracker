{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonevers/miniconda3/envs/adl4cv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from algorithms.feature_extraction_loading import FeatureDataset\n",
    "from algorithms.utils import feature_collate_fn\n",
    "#from learning_based.weighted_features_tracker import WeightedFeaturesTracker, WeightedHeatmapsTracker\n",
    "from math import ceil\n",
    "from evaluation.visualization import place_marker_in_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data\n",
    "\n",
    "dataset = FeatureDataset(\"features/davis/\")\n",
    "dataloader = DataLoader(dataset, 1, shuffle=True, collate_fn=feature_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_batch(query_points, target_points, occluded, trackgroup, batch_size=8, shuffle=True, drop_last=False):\n",
    "    \"\"\"\n",
    "    Yields a tuple containing one batch of query_points, target_points, occluded, trackgroup\n",
    "    \"\"\"\n",
    "    num_points = query_points.shape[0]\n",
    "\n",
    "    if shuffle:\n",
    "        permutation = np.random.permutation(num_points)\n",
    "        query_points = query_points[permutation]\n",
    "        target_points = target_points[permutation]\n",
    "        occluded = occluded[permutation]\n",
    "        trackgroup = trackgroup[permutation]\n",
    "\n",
    "    if drop_last:\n",
    "        num_batches = num_points // batch_size\n",
    "    else:\n",
    "        num_batches = ceil(num_points / batch_size)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start = i*batch_size\n",
    "        end = min((i+1)*batch_size, num_points)\n",
    "\n",
    "        yield (\n",
    "            query_points[start:end],\n",
    "            target_points[start:end],\n",
    "            occluded[start:end],\n",
    "            trackgroup[start:end],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LearnUpsampleTracker(\n",
       "  (heatmap_processor): HeatmapProcessor(\n",
       "    (softmax): Softmax(dim=-1)\n",
       "    (relu): ReLU()\n",
       "    (heatmap_processing_layers): ModuleDict(\n",
       "      (hid1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (hid2): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (hid3): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (hid4): Linear(in_features=1, out_features=16, bias=True)\n",
       "      (occ_out): Linear(in_features=16, out_features=1, bias=True)\n",
       "      (regression_hid): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (regression_out): Linear(in_features=128, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (softmax): Softmax(dim=None)\n",
       "  (relu): ReLU()\n",
       "  (conv1d_up8): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_up16): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_up32_1): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_up32_2): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_down16): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_down8): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_down4_1): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_down4_2): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_mid4): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_dec64): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_dec128): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_dec256_1): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv1d_dec256_2): Conv2d(10, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (refine_conv1): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (refine_conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from learning_based.learn_upsample_tracker import LearnUpsampleTracker\n",
    "\n",
    "model = LearnUpsampleTracker(next(iter(dataloader))[0][\"features\"])\n",
    "\n",
    "model.load_state_dict(torch.load('trained_upsample_1.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "## Train input\n",
    "device = \"cpu\"\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "    data = data[0]\n",
    "\n",
    "    ### Overfitting to single data point\n",
    "    data[\"query_points\"] = data[\"query_points\"][:,:1,:]\n",
    "    data[\"target_points\"] = data[\"target_points\"][:,:1,:]\n",
    "    data[\"occluded\"] = data[\"occluded\"][:,:1]\n",
    "    data[\"trackgroup\"] = data[\"trackgroup\"][:,:1]\n",
    "\n",
    "    feature_dict = data['features']\n",
    "    for block_name, block_feat_list in feature_dict.items():\n",
    "        for i in range(len(block_feat_list)):\n",
    "            feature_dict[block_name][i] = feature_dict[block_name][i].to(dtype=torch.float32).to(device)\n",
    "\n",
    "    tracks = []\n",
    "    occ = []\n",
    "    counter = 0\n",
    "    for query_batch in get_query_batch(data[\"query_points\"][0], data[\"target_points\"][0], data[\"occluded\"][0], data[\"trackgroup\"][0]):\n",
    "            print(counter)\n",
    "            counter += 1\n",
    "\n",
    "            query_points, target_points, occluded, trackgroup = query_batch\n",
    "\n",
    "            query_points = torch.tensor(query_points, dtype=torch.float32, device=device)\n",
    "            target_points = torch.tensor(target_points[..., [1, 0]], dtype=torch.float32, device=device)\n",
    "            occluded = torch.tensor(occluded, dtype=torch.float32, device=device)\n",
    "            trackgroup = torch.tensor(trackgroup, dtype=torch.float32, device=device)\n",
    "\n",
    "            pred_points, _ = model(feature_dict, query_points)\n",
    "            \n",
    "            occ.append(occluded)\n",
    "            tracks.append(pred_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_points = torch.cat(tracks)\n",
    "all_occluded = torch.cat(occ)\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "place_marker_in_frames(data[\"video\"], all_pred_points, all_occluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_block\n",
      "torch.Size([50, 10, 8, 8])\n",
      "torch.Size([50, 10, 16, 16])\n",
      "torch.Size([50, 10, 32, 32])\n",
      "torch.Size([50, 10, 32, 32])\n",
      "down_block\n",
      "torch.Size([50, 10, 16, 16])\n",
      "torch.Size([50, 10, 8, 8])\n",
      "torch.Size([50, 10, 4, 4])\n",
      "torch.Size([50, 10, 4, 4])\n",
      "mid_block\n",
      "torch.Size([50, 10, 4, 4])\n",
      "decoder_block\n",
      "torch.Size([50, 10, 64, 64])\n",
      "torch.Size([50, 10, 128, 128])\n",
      "torch.Size([50, 10, 256, 256])\n",
      "torch.Size([50, 10, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for name, blocklist in feature_dict.items():\n",
    "    print(name)\n",
    "    for map in blocklist:\n",
    "        print(map.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
